# URL Extraction Tool - Nested URL Extraction

## üéØ Overview

‡§Ø‡§π tool **BFS (Breadth-First Search)** approach ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§ï‡•á documentation pages ‡§∏‡•á **nested URLs** ‡§ï‡•ã step-by-step extract ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§

## ‚ú® Features

### 1. **Recursive URL Extraction**
- Main URL ‡§∏‡•á ‡§∂‡•Å‡§∞‡•Ç ‡§ï‡§∞‡§ï‡•á ‡§∏‡§≠‡•Ä nested URLs ‡§®‡§ø‡§ï‡§æ‡§≤‡§§‡§æ ‡§π‡•à
- Step-by-step ‡§π‡§∞ level ‡§ï‡•Ä URLs ‡§ï‡•ã process ‡§ï‡§∞‡§§‡§æ ‡§π‡•à
- ‡§ï‡•ã‡§à ‡§≠‡•Ä nested URL miss ‡§®‡§π‡•Ä‡§Ç ‡§π‡•ã‡§§‡§æ

### 2. **BFS (Breadth-First Search) Algorithm**
```
Level 0: Main URL
  ‚Üì
Level 1: Main URL ‡§Æ‡•á‡§Ç ‡§Æ‡§ø‡§≤‡•á ‡§∏‡§≠‡•Ä URLs
  ‚Üì
Level 2: Level 1 ‡§ï‡•Ä URLs ‡§Æ‡•á‡§Ç ‡§Æ‡§ø‡§≤‡•á ‡§∏‡§≠‡•Ä nested URLs
  ‚Üì
Level 3: Level 2 ‡§ï‡•Ä URLs ‡§Æ‡•á‡§Ç ‡§Æ‡§ø‡§≤‡•á ‡§∏‡§≠‡•Ä nested URLs
```

### 3. **Smart Controls**
- **Max Depth**: Maximum 5 levels ‡§§‡§ï ‡§ú‡§æ‡§§‡§æ ‡§π‡•à (configurable) - **INCREASED!**
- **Max URLs per Level**: ‡§π‡§∞ level ‡§∏‡•á maximum 200 URLs process ‡§ï‡§∞‡§§‡§æ ‡§π‡•à (configurable) - **INCREASED!**
- **All Links Processed**: ‡§π‡§∞ page ‡§ï‡•Ä ‡§∏‡§≠‡•Ä links extract ‡§π‡•ã‡§§‡•Ä ‡§π‡•à‡§Ç (no arbitrary limit)
- **Visited Tracking**: Duplicate URLs ‡§ï‡•ã skip ‡§ï‡§∞‡§§‡§æ ‡§π‡•à - O(1) lookup
- **Infinite Loop Prevention**: Already visited URLs ‡§ï‡•ã track ‡§ï‡§∞‡§§‡§æ ‡§π‡•à
- **Error Resilience**: Individual page errors ‡§∏‡•á ‡§™‡•Ç‡§∞‡•Ä crawling stop ‡§®‡§π‡•Ä‡§Ç ‡§π‡•ã‡§§‡•Ä

## üîß Configuration

```python
class URLExtractionContext(BaseModel):
    userId: str
    mainUrl: str
    timeout: int = 60  # seconds - INCREASED for deep crawling
    max_depth: int = 5  # Maximum recursion depth - INCREASED (was 3)
    max_urls_per_level: int = 200  # Max URLs per level - INCREASED (was 50)
```

### ‚ú® Recent Updates (v2.0)
- ‚úÖ **Max Depth: 3 ‚Üí 5** - ‡§Ö‡§¨ 5 levels ‡§§‡§ï ‡§ú‡§æ ‡§∏‡§ï‡§§‡§æ ‡§π‡•à!
- ‚úÖ **URLs per Level: 50 ‚Üí 200** - ‡§π‡§∞ level ‡§∏‡•á 200 URLs process ‡§π‡•ã‡§Ç‡§ó‡•á
- ‚úÖ **Timeout: 30s ‚Üí 60s** - ‡§ú‡•ç‡§Ø‡§æ‡§¶‡§æ time ‡§ï‡•á ‡§≤‡§ø‡§è pages fetch ‡§ï‡§∞ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç
- ‚úÖ **NO LIMIT on links per page** - ‡§Ö‡§¨ ‡§π‡§∞ page ‡§ï‡•Ä ‡§∏‡§≠‡•Ä links process ‡§π‡•ã‡§Ç‡§ó‡•Ä
- ‚úÖ **Relaxed filtering** - Language pages ‡§î‡§∞ subdomains ‡§≠‡•Ä include ‡§π‡•ã‡§Ç‡§ó‡•á
- ‚úÖ **Better error handling** - Errors ‡§∏‡•á ‡§™‡•Ç‡§∞‡•Ä crawling stop ‡§®‡§π‡•Ä‡§Ç ‡§π‡•ã‡§ó‡•Ä

## üìä Example Output

```json
{
  "topics": [
    {
      "id": "getting-started",
      "title": "Getting Started",
      "url": "https://example.com/docs/getting-started",
      "description": "[Level 1] Documentation page: Getting Started",
      "section": "Docs",
      "depth": 1
    },
    {
      "id": "getting-started-installation",
      "title": "Installation",
      "url": "https://example.com/docs/getting-started/installation",
      "description": "[Level 2] Documentation page: Installation",
      "section": "Docs",
      "depth": 2
    },
    {
      "id": "getting-started-installation-windows",
      "title": "Windows Installation",
      "url": "https://example.com/docs/getting-started/installation/windows",
      "description": "[Level 3] Documentation page: Windows Installation",
      "section": "Docs",
      "depth": 3
    }
  ],
  "mainUrl": "https://example.com/docs",
  "totalPages": 3,
  "maxDepth": 3
}
```

## üöÄ How It Works

### Step-by-Step Process:

1. **Initialization**
   ```python
   queue = [(main_url, 0)]  # (URL, depth)
   visited = {main_url}
   ```

2. **BFS Loop**
   ```python
   while queue:
       current_url, current_depth = queue.popleft()
       
       # Extract URLs from current page
       urls = extract_urls_from_html(current_url)
       
       # Add new URLs to queue
       for url in urls:
           if url not in visited:
               queue.append((url, current_depth + 1))
               visited.add(url)
   ```

3. **Level-by-Level Processing**
   - **Level 0**: `https://example.com/docs`
     - Finds: `/intro`, `/guides`, `/api`
   
   - **Level 1**: 
     - Process `/intro` ‚Üí Finds: `/intro/setup`, `/intro/quickstart`
     - Process `/guides` ‚Üí Finds: `/guides/basic`, `/guides/advanced`
     - Process `/api` ‚Üí Finds: `/api/reference`, `/api/examples`
   
   - **Level 2**:
     - Process `/intro/setup` ‚Üí Finds nested URLs
     - Process `/intro/quickstart` ‚Üí Finds nested URLs
     - ... ‡§î‡§∞ ‡§∏‡§≠‡•Ä Level 1 URLs ‡§ï‡•á nested URLs

4. **Smart Filtering**
   - Same domain ‡§ï‡•á URLs ‡§π‡•Ä process ‡§π‡•ã‡§§‡•á ‡§π‡•à‡§Ç
   - External links skip ‡§π‡•ã‡§§‡•á ‡§π‡•à‡§Ç
   - Anchor links (#) skip ‡§π‡•ã‡§§‡•á ‡§π‡•à‡§Ç
   - PDF, images, etc. skip ‡§π‡•ã‡§§‡•á ‡§π‡•à‡§Ç

## üé® Progress Logging

Tool execution ‡§ï‡•á ‡§¶‡•å‡§∞‡§æ‡§® detailed logs ‡§¶‡§ø‡§ñ‡§æ‡§§‡§æ ‡§π‡•à:

```
üîç Starting BFS URL extraction from: https://example.com/docs
‚öôÔ∏è  Max Depth: 5, Max URLs per level: 200

üìç Level 0 | Page 1/1: https://example.com/docs
‚úÖ Found 45 URLs at Level 0 from this page
‚ûï Added 45 new URLs to crawl queue

üìç Level 1 | Page 2/46: https://example.com/docs/intro
‚úÖ Found 23 URLs at Level 1 from this page
‚ûï Added 18 new URLs to crawl queue

üìç Level 1 | Page 3/63: https://example.com/docs/guides
‚úÖ Found 34 URLs at Level 1 from this page
‚ûï Added 28 new URLs to crawl queue

üìç Level 2 | Page 47/91: https://example.com/docs/intro/setup
‚úÖ Found 12 URLs at Level 2 from this page
‚ûï Added 10 new URLs to crawl queue

‚ú® BFS Complete!
üìä Total pages processed: 250
üìä Total URLs found: 847
üìä Errors encountered: 3
üìä URLs per level: {0: 45, 1: 156, 2: 321, 3: 225, 4: 100}
üìä Total unique pages visited: 850
```

## ‚ö° Performance Optimizations (v2.0 Enhanced)

1. **Timeout Management**: Flexible timeouts (60s main, 30s per page, 5s for titles)
2. **URL Limiting**: ‡§π‡§∞ level ‡§∏‡•á maximum 200 URLs (configurable) - **INCREASED!**
3. **Depth Limiting**: Maximum 5 levels ‡§§‡§ï (configurable) - **INCREASED!**
4. **NO Arbitrary Limits**: ‡§∏‡§≠‡•Ä page links process ‡§π‡•ã‡§§‡•Ä ‡§π‡•à‡§Ç (‡§™‡§π‡§≤‡•á 200 ‡§ï‡•Ä limit ‡§•‡•Ä)
5. **Title Extraction**: Anchor text use ‡§ï‡§∞‡§§‡§æ ‡§π‡•à (fast, no extra HTTP request)
6. **Visited Tracking**: O(1) lookup ‡§ï‡•á ‡§≤‡§ø‡§è Set use ‡§ï‡§∞‡§§‡§æ ‡§π‡•à
7. **Relaxed Filtering**: Minimal filtering - ‡§∏‡§ø‡§∞‡•ç‡§´ zaruri cheezon ko hi filter ‡§ï‡§∞‡§§‡§æ ‡§π‡•à
8. **Error Resilience**: Individual errors ‡§∏‡•á process stop ‡§®‡§π‡•Ä‡§Ç ‡§π‡•ã‡§§‡•Ä
9. **Progress Tracking**: Real-time progress ‡§ï‡•á ‡§∏‡§æ‡§• detailed logging
10. **Subdomain Support**: Same base domain ‡§ï‡•á subdomains ‡§≠‡•Ä crawl ‡§π‡•ã‡§§‡•á ‡§π‡•à‡§Ç

## üîí Safety Features

### 1. Infinite Loop Prevention
```python
visited = set()  # Track all visited URLs
if url in visited:
    continue  # Skip already visited URLs
```

### 2. Depth Control
```python
if current_depth > max_depth:
    continue  # Stop at max depth
```

### 3. URL Explosion Control (Enhanced)
```python
# Process up to 200 URLs per level (was 50)
urls_to_process = urls[:max_urls_per_level]

# But extract ALL links from each page (no per-page limit)
anchors = soup.find_all('a', href=True)  # ALL links
```

### 4. Error Handling
```python
try:
    urls = extract_urls_from_html(url)
except Exception as e:
    print(f"‚ùå Error: {e}")
    continue  # Skip failed URLs, continue with others
```

## üìù Usage Example

```python
from url_extraction import extract_urls_from_documentation

# Extract URLs with default settings
result = await extract_urls_from_documentation(
    ctx=context,
    url="https://example.com/docs"
)

# Parse result
data = json.loads(result)
print(f"Total pages found: {data['totalPages']}")
print(f"Max depth reached: {data['maxDepth']}")

# Access topics
for topic in data['topics']:
    print(f"[Level {topic['depth']}] {topic['title']}: {topic['url']}")
```

## üéØ Key Benefits

1. ‚úÖ **Complete Coverage**: ‡§∏‡§≠‡•Ä nested URLs ‡§Æ‡§ø‡§≤‡§§‡•á ‡§π‡•à‡§Ç, ‡§ï‡•ã‡§à miss ‡§®‡§π‡•Ä‡§Ç ‡§π‡•ã‡§§‡§æ
2. ‚úÖ **Step-by-Step**: Proper hierarchical order ‡§Æ‡•á‡§Ç URLs extract ‡§π‡•ã‡§§‡•á ‡§π‡•à‡§Ç
3. ‚úÖ **Safe**: Infinite loops ‡§î‡§∞ timeouts ‡§∏‡•á protected
4. ‚úÖ **Efficient**: Smart filtering ‡§î‡§∞ limiting ‡§ï‡•á ‡§∏‡§æ‡§•
5. ‚úÖ **Transparent**: Detailed progress logging ‡§ï‡•á ‡§∏‡§æ‡§•
6. ‚úÖ **Configurable**: Depth ‡§î‡§∞ URLs per level customize ‡§ï‡§∞ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç

## üîç Comparison: Old vs New

### ‚ùå Old Approach (Single Level)
```
Main URL ‚Üí Extract URLs ‚Üí Done
```
- ‡§∏‡§ø‡§∞‡•ç‡§´ main page ‡§ï‡•Ä URLs ‡§Æ‡§ø‡§≤‡§§‡•Ä ‡§•‡•Ä‡§Ç
- Nested URLs miss ‡§π‡•ã ‡§ú‡§æ‡§§‡•Ä ‡§•‡•Ä‡§Ç

### ‚úÖ New Approach (BFS - Recursive)
```
Main URL
  ‚Üí Level 1 URLs
    ‚Üí Level 2 URLs
      ‚Üí Level 3 URLs
        ‚Üí Done
```
- ‡§∏‡§≠‡•Ä nested URLs step-by-step ‡§Æ‡§ø‡§≤‡§§‡•Ä ‡§π‡•à‡§Ç
- Complete documentation coverage
- Hierarchical structure maintain ‡§π‡•ã‡§§‡•Ä ‡§π‡•à

## üõ†Ô∏è Customization

‡§Ö‡§ó‡§∞ ‡§Ü‡§™‡§ï‡•ã ‡§Ö‡§≤‡§ó settings ‡§ö‡§æ‡§π‡§ø‡§è:

### üî• Aggressive Crawling (Maximum Coverage)
```python
context = URLExtractionContext(
    userId="user123",
    mainUrl="https://example.com/docs",
    timeout=120,         # 2 minutes timeout
    max_depth=7,         # 7 levels ‡§§‡§ï ‡§ú‡§æ‡§è‡§Ç!
    max_urls_per_level=500  # ‡§π‡§∞ level ‡§∏‡•á 500 URLs
)
```

### ‚öñÔ∏è Balanced (Default - Recommended)
```python
context = URLExtractionContext(
    userId="user123",
    mainUrl="https://example.com/docs",
    timeout=60,          # 60 seconds (default)
    max_depth=5,         # 5 levels (default)
    max_urls_per_level=200  # 200 URLs (default)
)
```

### üèÉ Quick Crawl (Fast but Limited)
```python
context = URLExtractionContext(
    userId="user123",
    mainUrl="https://example.com/docs",
    timeout=30,          # 30 seconds
    max_depth=3,         # 3 levels only
    max_urls_per_level=50  # ‡§π‡§∞ level ‡§∏‡•á 50 URLs
)
```

---

**Made with ‚ù§Ô∏è for Complete Documentation Coverage**

